#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
íŠ¹í—ˆ ë°ì´í„° ì²˜ë¦¬ ë° ì „ì²˜ë¦¬ ëª¨ë“ˆ
Patent Data Processing and Preprocessing Module

íŠ¹í—ˆ ë¬¸ì„œì˜ í…ìŠ¤íŠ¸, ë„ë©´, ë©”íƒ€ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ê³  LLM í›ˆë ¨ì— ì í•©í•œ í˜•íƒœë¡œ ë³€í™˜

MIT License
Copyright (c) 2025
"""

import os
import re
import json
import sqlite3
import requests
import logging
from typing import List, Dict, Optional, Tuple, Any
from dataclasses import dataclass
import xml.etree.ElementTree as ET
from pathlib import Path
import base64
from datetime import datetime
import zipfile
import pandas as pd
import numpy as np
from PIL import Image
import pytesseract
import cv2

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class ProcessedPatentData:
    """ì²˜ë¦¬ëœ íŠ¹í—ˆ ë°ì´í„° í´ë˜ìŠ¤"""
    patent_id: str
    processed_text: str
    structured_claims: List[Dict]
    extracted_figures: List[Dict]
    technical_keywords: List[str]
    classification_hierarchy: Dict
    processing_metadata: Dict

class PatentTextProcessor:
    """íŠ¹í—ˆ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ê¸°"""
    
    def __init__(self):
        # íŠ¹í—ˆ ì „ìš© ì •ê·œí‘œí˜„ì‹ íŒ¨í„´
        self.claim_pattern = r'(?:Claim|CLAIM)\s*(\d+)\.?\s*(.+?)(?=(?:Claim|CLAIM)\s*\d+|$)'
        self.figure_ref_pattern = r'(?:FIG\.|Figure|Fig\.)\s*(\d+[A-Za-z]?)'
        self.patent_id_pattern = r'(?:US|EP|WO|KR|JP|CN)\s*\d+\s*[A-Z]\d*'
        
        # ê¸°ìˆ  ë¶„ì•¼ë³„ í‚¤ì›Œë“œ ì‚¬ì „
        self.tech_keywords = {
            'ai_ml': ['artificial intelligence', 'machine learning', 'neural network', 'deep learning', 'algorithm'],
            'electronics': ['circuit', 'semiconductor', 'transistor', 'voltage', 'current', 'resistance'],
            'mechanical': ['mechanism', 'gear', 'bearing', 'actuator', 'motor', 'transmission'],
            'chemical': ['compound', 'reaction', 'catalyst', 'synthesis', 'polymer', 'molecule'],
            'biotech': ['protein', 'gene', 'dna', 'enzyme', 'cell', 'biological'],
            'software': ['software', 'program', 'code', 'application', 'system', 'method']
        }
    
    def clean_patent_text(self, raw_text: str) -> str:
        """íŠ¹í—ˆ í…ìŠ¤íŠ¸ ì •ë¦¬"""
        # ë¶ˆí•„ìš”í•œ ë¬¸ì ì œê±°
        text = re.sub(r'\s+', ' ', raw_text)  # ì—°ì† ê³µë°± ì œê±°
        text = re.sub(r'\n+', '\n', text)     # ì—°ì† ì¤„ë°”ê¿ˆ ì œê±°
        text = re.sub(r'[^\w\s\.\,\;\:\(\)\[\]\-\+\=\<\>\/\\\"\']', '', text)  # íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬
        
        # íŠ¹í—ˆ íŠ¹ìœ ì˜ í‘œí˜„ ì •ë¦¬
        text = re.sub(r'(?i)background\s+of\s+the\s+invention', 'BACKGROUND', text)
        text = re.sub(r'(?i)summary\s+of\s+the\s+invention', 'SUMMARY', text)
        text = re.sub(r'(?i)detailed\s+description', 'DETAILED_DESCRIPTION', text)
        text = re.sub(r'(?i)brief\s+description\s+of\s+the\s+drawings', 'DRAWINGS_DESCRIPTION', text)
        
        return text.strip()
    
    def extract_claims(self, patent_text: str) -> List[Dict]:
        """íŠ¹í—ˆ í´ë ˆì„ ì¶”ì¶œ ë° êµ¬ì¡°í™”"""
        claims = []
        
        # í´ë ˆì„ ì„¹ì…˜ ì°¾ê¸°
        claim_section_match = re.search(r'(?i)(?:claims?|what\s+is\s+claimed)\s*:?\s*(.*?)(?:abstract|drawings|$)', 
                                      patent_text, re.DOTALL)
        
        if claim_section_match:
            claim_text = claim_section_match.group(1)
            
            # ê°œë³„ í´ë ˆì„ ì¶”ì¶œ
            claim_matches = re.finditer(self.claim_pattern, claim_text, re.DOTALL | re.IGNORECASE)
            
            for match in claim_matches:
                claim_num = int(match.group(1))
                claim_content = match.group(2).strip()
                
                # í´ë ˆì„ íƒ€ì… ë¶„ë¥˜
                claim_type = self._classify_claim_type(claim_content)
                
                # ì¢…ì† í´ë ˆì„ ê´€ê³„ íŒŒì•…
                dependency = self._extract_claim_dependency(claim_content)
                
                claims.append({
                    'claim_number': claim_num,
                    'content': claim_content,
                    'type': claim_type,
                    'dependency': dependency,
                    'length': len(claim_content.split()),
                    'technical_elements': self._extract_technical_elements(claim_content)
                })
        
        return sorted(claims, key=lambda x: x['claim_number'])
    
    def _classify_claim_type(self, claim_content: str) -> str:
        """í´ë ˆì„ íƒ€ì… ë¶„ë¥˜"""
        content_lower = claim_content.lower()
        
        if any(keyword in content_lower for keyword in ['method', 'process', 'step', 'procedure']):
            return 'method'
        elif any(keyword in content_lower for keyword in ['system', 'apparatus', 'device', 'machine']):
            return 'apparatus'
        elif any(keyword in content_lower for keyword in ['composition', 'compound', 'material']):
            return 'composition'
        else:
            return 'other'
    
    def _extract_claim_dependency(self, claim_content: str) -> Optional[int]:
        """ì¢…ì† í´ë ˆì„ ê´€ê³„ ì¶”ì¶œ"""
        dependency_match = re.search(r'(?:according\s+to|of)\s+claim\s+(\d+)', claim_content, re.IGNORECASE)
        if dependency_match:
            return int(dependency_match.group(1))
        return None
    
    def _extract_technical_elements(self, claim_content: str) -> List[str]:
        """ê¸°ìˆ ì  ìš”ì†Œ ì¶”ì¶œ"""
        elements = []
        
        # ëª…ì‚¬êµ¬ ì¶”ì¶œ (ê°„ë‹¨í•œ íŒ¨í„´)
        noun_phrases = re.findall(r'\b(?:[A-Z][a-z]+\s+)*[A-Z][a-z]+\b', claim_content)
        elements.extend(noun_phrases)
        
        # ê¸°ìˆ  í‚¤ì›Œë“œ ë§¤ì¹­
        for category, keywords in self.tech_keywords.items():
            for keyword in keywords:
                if keyword.lower() in claim_content.lower():
                    elements.append(f"{category}:{keyword}")
        
        return list(set(elements))
    
    def extract_technical_keywords(self, patent_text: str) -> List[str]:
        """ê¸°ìˆ  í‚¤ì›Œë“œ ì¶”ì¶œ"""
        keywords = []
        text_lower = patent_text.lower()
        
        # ê¸°ìˆ  ë¶„ì•¼ë³„ í‚¤ì›Œë“œ ì¶”ì¶œ
        for category, keyword_list in self.tech_keywords.items():
            for keyword in keyword_list:
                if keyword in text_lower:
                    keywords.append(keyword)
        
        # TF-IDF ê¸°ë°˜ ì¤‘ìš” ë‹¨ì–´ ì¶”ì¶œ (ê°„ë‹¨í•œ êµ¬í˜„)
        words = re.findall(r'\b[a-zA-Z]{3,}\b', patent_text.lower())
        word_freq = {}
        for word in words:
            word_freq[word] = word_freq.get(word, 0) + 1
        
        # ë¹ˆë„ ê¸°ë°˜ ìƒìœ„ í‚¤ì›Œë“œ ì¶”ì¶œ
        frequent_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:20]
        keywords.extend([word for word, freq in frequent_words if freq > 2])
        
        return list(set(keywords))
    
    def structure_patent_sections(self, patent_text: str) -> Dict[str, str]:
        """íŠ¹í—ˆ ì„¹ì…˜ êµ¬ì¡°í™”"""
        sections = {}
        
        # ì£¼ìš” ì„¹ì…˜ íŒ¨í„´
        section_patterns = {
            'title': r'(?i)(?:title|invention\s+title)\s*:?\s*(.+?)(?:\n|$)',
            'abstract': r'(?i)abstract\s*:?\s*(.*?)(?=background|field|summary|$)',
            'background': r'(?i)background\s*(?:of\s+the\s+invention)?\s*:?\s*(.*?)(?=summary|field|detailed|$)',
            'summary': r'(?i)summary\s*(?:of\s+the\s+invention)?\s*:?\s*(.*?)(?=detailed|drawings|claims|$)',
            'detailed_description': r'(?i)detailed\s+description\s*:?\s*(.*?)(?=claims|drawings|$)',
            'claims': r'(?i)claims?\s*:?\s*(.*?)(?=abstract|drawings|$)'
        }
        
        for section_name, pattern in section_patterns.items():
            match = re.search(pattern, patent_text, re.DOTALL)
            if match:
                sections[section_name] = match.group(1).strip()
        
        return sections

class PatentFigureProcessor:
    """íŠ¹í—ˆ ë„ë©´ ì²˜ë¦¬ê¸°"""
    
    def __init__(self):
        self.supported_formats = ['.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.pdf']
        
    def extract_figures_from_pdf(self, pdf_path: str, output_dir: str) -> List[str]:
        """PDFì—ì„œ ë„ë©´ ì¶”ì¶œ"""
        figure_paths = []
        
        try:
            # PDFì—ì„œ ì´ë¯¸ì§€ ì¶”ì¶œ (pdf2image ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© ê°€ì •)
            import pdf2image
            
            pages = pdf2image.convert_from_path(pdf_path)
            
            for i, page in enumerate(pages):
                figure_path = os.path.join(output_dir, f"figure_{i+1}.png")
                page.save(figure_path, 'PNG')
                figure_paths.append(figure_path)
                
        except ImportError:
            logger.warning("pdf2image ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        except Exception as e:
            logger.error(f"PDF ë„ë©´ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        
        return figure_paths
    
    def analyze_figure_content(self, image_path: str) -> Dict:
        """ë„ë©´ ë‚´ìš© ë¶„ì„"""
        try:
            # ì´ë¯¸ì§€ ë¡œë“œ
            image = cv2.imread(image_path)
            if image is None:
                return {"error": "ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨"}
            
            # ì´ë¯¸ì§€ ê¸°ë³¸ ì •ë³´
            height, width = image.shape[:2]
            
            # OCRì„ í†µí•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            text = pytesseract.image_to_string(Image.open(image_path))
            
            # ë„ë©´ ìš”ì†Œ ë¶„ì„
            analysis = {
                "image_path": image_path,
                "dimensions": {"width": width, "height": height},
                "extracted_text": text.strip(),
                "figure_numbers": self._extract_figure_numbers(text),
                "reference_numerals": self._extract_reference_numerals(text),
                "diagram_type": self._classify_diagram_type(image, text),
                "complexity_score": self._calculate_complexity_score(image)
            }
            
            return analysis
            
        except Exception as e:
            logger.error(f"ë„ë©´ ë¶„ì„ ì˜¤ë¥˜: {e}")
            return {"error": str(e)}
    
    def _extract_figure_numbers(self, text: str) -> List[str]:
        """ë„ë©´ ë²ˆí˜¸ ì¶”ì¶œ"""
        figure_numbers = re.findall(r'(?:FIG\.|Figure|Fig\.)\s*(\d+[A-Za-z]?)', text, re.IGNORECASE)
        return list(set(figure_numbers))
    
    def _extract_reference_numerals(self, text: str) -> List[str]:
        """ì°¸ì¡° ë²ˆí˜¸ ì¶”ì¶œ"""
        # ì¼ë°˜ì ìœ¼ë¡œ íŠ¹í—ˆ ë„ë©´ì—ì„œ ì‚¬ìš©ë˜ëŠ” ì°¸ì¡° ë²ˆí˜¸ íŒ¨í„´
        numerals = re.findall(r'\b(\d{1,3})\b', text)
        return [num for num in set(numerals) if 10 <= int(num) <= 999]
    
    def _classify_diagram_type(self, image: np.ndarray, text: str) -> str:
        """ë„ë©´ íƒ€ì… ë¶„ë¥˜"""
        text_lower = text.lower()
        
        # í…ìŠ¤íŠ¸ ê¸°ë°˜ ë¶„ë¥˜
        if any(keyword in text_lower for keyword in ['circuit', 'schematic', 'electrical']):
            return 'circuit_diagram'
        elif any(keyword in text_lower for keyword in ['flow', 'process', 'method']):
            return 'flowchart'
        elif any(keyword in text_lower for keyword in ['block', 'system', 'architecture']):
            return 'block_diagram'
        
        # ì´ë¯¸ì§€ ê¸°ë°˜ ë¶„ë¥˜ (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        edges = cv2.Canny(gray, 50, 150)
        
        # ì§ì„  ê²€ì¶œ
        lines = cv2.HoughLinesP(edges, 1, np.pi/180, threshold=100, minLineLength=50, maxLineGap=10)
        line_count = len(lines) if lines is not None else 0
        
        # ì› ê²€ì¶œ
        circles = cv2.HoughCircles(gray, cv2.HOUGH_GRADIENT, 1, 20, param1=50, param2=30, minRadius=5, maxRadius=100)
        circle_count = len(circles[0]) if circles is not None else 0
        
        if line_count > 20 and circle_count > 5:
            return 'technical_drawing'
        elif line_count > 10:
            return 'schematic'
        else:
            return 'illustration'
    
    def _calculate_complexity_score(self, image: np.ndarray) -> float:
        """ë„ë©´ ë³µì¡ë„ ì ìˆ˜ ê³„ì‚°"""
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        
        # ì—£ì§€ ë°€ë„ ê³„ì‚°
        edges = cv2.Canny(gray, 50, 150)
        edge_density = np.sum(edges > 0) / (edges.shape[0] * edges.shape[1])
        
        # í…ìŠ¤ì²˜ ë³µì¡ë„ (í‘œì¤€í¸ì°¨ ê¸°ë°˜)
        texture_complexity = np.std(gray) / 255.0
        
        # ì „ì²´ ë³µì¡ë„ ì ìˆ˜ (0-1 ë²”ìœ„)
        complexity_score = (edge_density * 0.7 + texture_complexity * 0.3)
        
        return min(complexity_score, 1.0)

class PatentDataCollector:
    """íŠ¹í—ˆ ë°ì´í„° ìˆ˜ì§‘ê¸°"""
    
    def __init__(self, data_dir: str = "patent_data"):
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(exist_ok=True)
        
        # ë°ì´í„° ì†ŒìŠ¤ ì„¤ì •
        self.data_sources = {
            'uspto': {
                'bulk_data_url': 'https://data.uspto.gov/bulkdata',
                'api_url': 'https://developer.uspto.gov/api',
                'formats': ['xml', 'json']
            },
            'epo': {
                'ops_url': 'https://ops.epo.org/3.2',
                'espacenet_url': 'https://worldwide.espacenet.com',
                'formats': ['xml', 'json']
            },
            'kipo': {
                'kipris_url': 'https://www.kipris.or.kr',
                'formats': ['xml', 'html']
            }
        }
    
    def collect_uspto_bulk_data(self, year: int, data_type: str = 'grant') -> str:
        """USPTO ëŒ€ëŸ‰ ë°ì´í„° ìˆ˜ì§‘"""
        try:
            # ëŒ€ëŸ‰ ë°ì´í„° ë‹¤ìš´ë¡œë“œ URL êµ¬ì„±
            if data_type == 'grant':
                url = f"https://data.uspto.gov/bulkdata/patent/grant/redbook/fulltext/{year}/"
            else:
                url = f"https://data.uspto.gov/bulkdata/patent/application/redbook/fulltext/{year}/"
            
            # ë°ì´í„° ë‹¤ìš´ë¡œë“œ ë””ë ‰í† ë¦¬ ìƒì„±
            download_dir = self.data_dir / f"uspto_{data_type}_{year}"
            download_dir.mkdir(exist_ok=True)
            
            logger.info(f"USPTO {data_type} ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘: {year}ë…„")
            
            # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” requestsë¥¼ ì‚¬ìš©í•˜ì—¬ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
            # ì—¬ê¸°ì„œëŠ” ì˜ˆì‹œ êµ¬ì¡°ë§Œ ì œê³µ
            
            return str(download_dir)
            
        except Exception as e:
            logger.error(f"USPTO ë°ì´í„° ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
            return ""
    
    def parse_uspto_xml(self, xml_file_path: str) -> List[Dict]:
        """USPTO XML íŒŒì¼ íŒŒì‹±"""
        patents = []
        
        try:
            tree = ET.parse(xml_file_path)
            root = tree.getroot()
            
            # XML êµ¬ì¡°ì— ë”°ë¼ íŠ¹í—ˆ ì •ë³´ ì¶”ì¶œ
            for patent_elem in root.findall('.//us-patent-grant'):
                patent_data = self._extract_patent_from_xml(patent_elem)
                if patent_data:
                    patents.append(patent_data)
            
        except Exception as e:
            logger.error(f"XML íŒŒì‹± ì˜¤ë¥˜: {e}")
        
        return patents
    
    def _extract_patent_from_xml(self, patent_elem: ET.Element) -> Optional[Dict]:
        """XML ìš”ì†Œì—ì„œ íŠ¹í—ˆ ì •ë³´ ì¶”ì¶œ"""
        try:
            # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
            patent_id = patent_elem.find('.//doc-number')
            title = patent_elem.find('.//invention-title')
            abstract = patent_elem.find('.//abstract')
            
            # í´ë ˆì„ ì¶”ì¶œ
            claims = []
            for claim_elem in patent_elem.findall('.//claim'):
                claim_text = ET.tostring(claim_elem, encoding='unicode', method='text')
                claims.append(claim_text.strip())
            
            # ë°œëª…ì ì •ë³´ ì¶”ì¶œ
            inventors = []
            for inventor_elem in patent_elem.findall('.//inventor'):
                name_elem = inventor_elem.find('.//name')
                if name_elem is not None:
                    inventors.append(ET.tostring(name_elem, encoding='unicode', method='text').strip())
            
            patent_data = {
                'patent_id': patent_id.text if patent_id is not None else '',
                'title': title.text if title is not None else '',
                'abstract': ET.tostring(abstract, encoding='unicode', method='text').strip() if abstract is not None else '',
                'claims': claims,
                'inventors': inventors,
                'source': 'uspto'
            }
            
            return patent_data
            
        except Exception as e:
            logger.error(f"íŠ¹í—ˆ ì •ë³´ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return None

class PatentDataPreprocessor:
    """íŠ¹í—ˆ ë°ì´í„° ì „ì²˜ë¦¬ê¸° í†µí•© í´ë˜ìŠ¤"""
    
    def __init__(self, output_db_path: str = "processed_patents.db"):
        self.text_processor = PatentTextProcessor()
        self.figure_processor = PatentFigureProcessor()
        self.data_collector = PatentDataCollector()
        
        self.output_db_path = output_db_path
        self.init_database()
    
    def init_database(self):
        """ì „ì²˜ë¦¬ ê²°ê³¼ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        conn = sqlite3.connect(self.output_db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS processed_patents (
                patent_id TEXT PRIMARY KEY,
                processed_text TEXT,
                structured_claims TEXT,
                extracted_figures TEXT,
                technical_keywords TEXT,
                classification_hierarchy TEXT,
                processing_metadata TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        conn.commit()
        conn.close()
    
    def process_patent_document(self, patent_data: Dict, figure_dir: Optional[str] = None) -> ProcessedPatentData:
        """íŠ¹í—ˆ ë¬¸ì„œ ì „ì²´ ì²˜ë¦¬"""
        
        logger.info(f"íŠ¹í—ˆ ë¬¸ì„œ ì²˜ë¦¬ ì‹œì‘: {patent_data.get('patent_id', 'Unknown')}")
        
        # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        raw_text = f"{patent_data.get('title', '')} {patent_data.get('abstract', '')} {' '.join(patent_data.get('claims', []))}"
        processed_text = self.text_processor.clean_patent_text(raw_text)
        
        # í´ë ˆì„ êµ¬ì¡°í™”
        structured_claims = self.text_processor.extract_claims(raw_text)
        
        # ê¸°ìˆ  í‚¤ì›Œë“œ ì¶”ì¶œ
        technical_keywords = self.text_processor.extract_technical_keywords(processed_text)
        
        # ë„ë©´ ì²˜ë¦¬
        extracted_figures = []
        if figure_dir and os.path.exists(figure_dir):
            for figure_file in os.listdir(figure_dir):
                if any(figure_file.lower().endswith(ext) for ext in self.figure_processor.supported_formats):
                    figure_path = os.path.join(figure_dir, figure_file)
                    figure_analysis = self.figure_processor.analyze_figure_content(figure_path)
                    extracted_figures.append(figure_analysis)
        
        # ë¶„ë¥˜ ê³„ì¸µ êµ¬ì¡° ìƒì„±
        classification_hierarchy = self._build_classification_hierarchy(patent_data, technical_keywords)
        
        # ì²˜ë¦¬ ë©”íƒ€ë°ì´í„°
        processing_metadata = {
            'processing_date': datetime.now().isoformat(),
            'text_length': len(processed_text),
            'claim_count': len(structured_claims),
            'figure_count': len(extracted_figures),
            'keyword_count': len(technical_keywords)
        }
        
        processed_data = ProcessedPatentData(
            patent_id=patent_data.get('patent_id', ''),
            processed_text=processed_text,
            structured_claims=structured_claims,
            extracted_figures=extracted_figures,
            technical_keywords=technical_keywords,
            classification_hierarchy=classification_hierarchy,
            processing_metadata=processing_metadata
        )
        
        # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
        self._save_processed_data(processed_data)
        
        logger.info(f"íŠ¹í—ˆ ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ: {processed_data.patent_id}")
        return processed_data
    
    def _build_classification_hierarchy(self, patent_data: Dict, keywords: List[str]) -> Dict:
        """ë¶„ë¥˜ ê³„ì¸µ êµ¬ì¡° êµ¬ì¶•"""
        hierarchy = {
            'primary_category': 'unknown',
            'secondary_categories': [],
            'technical_domains': [],
            'application_areas': []
        }
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ë¥˜
        keyword_categories = {
            'ai_ml': ['artificial intelligence', 'machine learning', 'neural network'],
            'electronics': ['circuit', 'semiconductor', 'electronic'],
            'mechanical': ['mechanical', 'motor', 'gear'],
            'software': ['software', 'algorithm', 'program'],
            'biotech': ['biological', 'medical', 'pharmaceutical']
        }
        
        for category, category_keywords in keyword_categories.items():
            if any(kw in ' '.join(keywords).lower() for kw in category_keywords):
                if hierarchy['primary_category'] == 'unknown':
                    hierarchy['primary_category'] = category
                else:
                    hierarchy['secondary_categories'].append(category)
        
        return hierarchy
    
    def _save_processed_data(self, processed_data: ProcessedPatentData):
        """ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥"""
        conn = sqlite3.connect(self.output_db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT OR REPLACE INTO processed_patents 
            (patent_id, processed_text, structured_claims, extracted_figures,
             technical_keywords, classification_hierarchy, processing_metadata)
            VALUES (?, ?, ?, ?, ?, ?, ?)
        ''', (
            processed_data.patent_id,
            processed_data.processed_text,
            json.dumps(processed_data.structured_claims),
            json.dumps(processed_data.extracted_figures),
            json.dumps(processed_data.technical_keywords),
            json.dumps(processed_data.classification_hierarchy),
            json.dumps(processed_data.processing_metadata)
        ))
        
        conn.commit()
        conn.close()
    
    def batch_process_patents(self, patent_data_list: List[Dict]) -> List[ProcessedPatentData]:
        """íŠ¹í—ˆ ë°ì´í„° ë°°ì¹˜ ì²˜ë¦¬"""
        processed_patents = []
        
        for i, patent_data in enumerate(patent_data_list):
            try:
                logger.info(f"ë°°ì¹˜ ì²˜ë¦¬ ì§„í–‰: {i+1}/{len(patent_data_list)}")
                processed_patent = self.process_patent_document(patent_data)
                processed_patents.append(processed_patent)
                
            except Exception as e:
                logger.error(f"íŠ¹í—ˆ ì²˜ë¦¬ ì˜¤ë¥˜ ({patent_data.get('patent_id', 'Unknown')}): {e}")
        
        logger.info(f"ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: {len(processed_patents)}ê°œ íŠ¹í—ˆ ì²˜ë¦¬ë¨")
        return processed_patents
    
    def get_processing_statistics(self) -> Dict:
        """ì²˜ë¦¬ í†µê³„ ì¡°íšŒ"""
        conn = sqlite3.connect(self.output_db_path)
        cursor = conn.cursor()
        
        cursor.execute("SELECT COUNT(*) FROM processed_patents")
        total_patents = cursor.fetchone()[0]
        
        cursor.execute("SELECT AVG(json_extract(processing_metadata, '$.text_length')) FROM processed_patents")
        avg_text_length = cursor.fetchone()[0] or 0
        
        cursor.execute("SELECT AVG(json_extract(processing_metadata, '$.claim_count')) FROM processed_patents")
        avg_claim_count = cursor.fetchone()[0] or 0
        
        conn.close()
        
        return {
            'total_processed_patents': total_patents,
            'average_text_length': round(avg_text_length, 2),
            'average_claim_count': round(avg_claim_count, 2)
        }

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("=" * 80)
    print("ğŸ”§ íŠ¹í—ˆ ë°ì´í„° ì²˜ë¦¬ ë° ì „ì²˜ë¦¬ ì‹œìŠ¤í…œ")
    print("Patent Data Processing and Preprocessing System")
    print("=" * 80)
    
    # ì „ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
    preprocessor = PatentDataPreprocessor()
    
    while True:
        print("\nğŸ“‹ ë©”ë‰´ë¥¼ ì„ íƒí•˜ì„¸ìš”:")
        print("1. ğŸ“„ ë‹¨ì¼ íŠ¹í—ˆ ë¬¸ì„œ ì²˜ë¦¬")
        print("2. ğŸ“¦ ë°°ì¹˜ íŠ¹í—ˆ ë°ì´í„° ì²˜ë¦¬")
        print("3. ğŸ“Š ì²˜ë¦¬ í†µê³„ ì¡°íšŒ")
        print("4. ğŸ” USPTO ë°ì´í„° ìˆ˜ì§‘")
        print("5. âŒ ì¢…ë£Œ")
        
        choice = input("\nì„ íƒ (1-5): ").strip()
        
        if choice == '1':
            # ìƒ˜í”Œ íŠ¹í—ˆ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸
            sample_patent = {
                'patent_id': 'US1234567A',
                'title': 'Artificial Intelligence System for Patent Analysis',
                'abstract': 'This invention relates to an artificial intelligence system that analyzes patent documents using machine learning algorithms.',
                'claims': [
                    'A method for analyzing patent documents comprising: receiving patent text, processing with neural networks, generating analysis results.',
                    'The method of claim 1, wherein the neural network is a transformer-based language model.'
                ]
            }
            
            processed = preprocessor.process_patent_document(sample_patent)
            print(f"\nâœ… íŠ¹í—ˆ ì²˜ë¦¬ ì™„ë£Œ: {processed.patent_id}")
            print(f"   - ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(processed.processed_text)}")
            print(f"   - êµ¬ì¡°í™”ëœ í´ë ˆì„: {len(processed.structured_claims)}ê°œ")
            print(f"   - ê¸°ìˆ  í‚¤ì›Œë“œ: {len(processed.technical_keywords)}ê°œ")
        
        elif choice == '2':
            print("ë°°ì¹˜ ì²˜ë¦¬ ê¸°ëŠ¥ì€ ì‹¤ì œ íŠ¹í—ˆ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            
        elif choice == '3':
            stats = preprocessor.get_processing_statistics()
            print(f"\nğŸ“Š ì²˜ë¦¬ í†µê³„:")
            print(f"- ì´ ì²˜ë¦¬ëœ íŠ¹í—ˆ: {stats['total_processed_patents']}ê°œ")
            print(f"- í‰ê·  í…ìŠ¤íŠ¸ ê¸¸ì´: {stats['average_text_length']}")
            print(f"- í‰ê·  í´ë ˆì„ ìˆ˜: {stats['average_claim_count']}")
        
        elif choice == '4':
            year = input("ìˆ˜ì§‘í•  ì—°ë„ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: 2023): ").strip()
            if year.isdigit():
                collector = PatentDataCollector()
                result_dir = collector.collect_uspto_bulk_data(int(year))
                print(f"âœ… ë°ì´í„° ìˆ˜ì§‘ ë””ë ‰í† ë¦¬: {result_dir}")
            else:
                print("âŒ ì˜¬ë°”ë¥¸ ì—°ë„ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        
        elif choice == '5':
            print("ì‹œìŠ¤í…œì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        
        else:
            print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")

if __name__ == "__main__":
    main()

